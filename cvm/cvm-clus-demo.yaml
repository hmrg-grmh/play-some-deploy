---
## 身份：
## organizer
## dancer

## (Dirigent)

## 实体可能是同一个实体也可能不是
## 实体就是 node





################################################






## Mysql-meta:
## addin_meta_mysql ()
## {
##     out_sql_md () { echo 'create database if not exists `{}` ;' && echo 'grant all on `{}`.* to "{}"@"%" identified by "{}-pass&" ;' && echo 'show grants for "{}"@"%" ;' ; } &&
##     (echo "$@" | xargs -n1) |
##         xargs -i -P0 -- /usr/bin/sh -c "$(declare -f out_sql_md)"' && out_sql_md | mysql -uroot -p123456' ;
## } ; addin_meta_mysql hive am oozie hue spark hbase 

## CDH - HDFS 
## CDH 的 DN dfs.datanode.data.dir 默认会找上所有的挂载盘，试图用来做数据存储。
## 一定要注意删掉那些为了共享而挂给 Pod 的卷。
## 除此之外的默认就是 /run/secrets 下的 dfs/dn 了。
## dfs.namenode.name.dir 则默认优先只会找挂载卷的 dfs/nn ， dfs.namenode.checkpoint.dir 则默认就找 /run/secrets 的 dfs/snn 。
## dfs.datanode.data.dir 【数据目录】的数量要大于不等于【接受的 DataNode 失败的卷】 dfs.datanode.failed.volumes.tolerated 的指定值。

## CDH - Yarn
## yarn.nodemanager.local-dirs 和上面的 DN 一样，
## 有两个组，默认组是 /run/secrets 下有 yarn/nm ，二组是除了 /run/secrets 下有 yarn/nm 还有挂载目录下有 yarn/nm ；
## 注意不是数据挂载目录的话手动删了。

## CDH - ZK
## dataDir 默认 /var/lib/zookeeper
## dataLogDir 默认 /var/lib/zookeeper

## 配置：

## dfs.datanode.max.locked.memory 是【用于缓存的最大内存】
## 但容器内的系统的这个 limit 值只有 65536 Bytes（ 64 KiB ）。要调小。
## 上面的问题看角色日志里红色背景的部分。

## 顺序：

## 安装和启动的顺序 zk, yarn & hdfs, spark, hive

## 装好后启动前需要做的：
## sudo -u hdfs -- hdfs dfs -mkdir -p /tmp /user
## sudo -u hdfs -- hdfs dfs -chmod 1777 /tmp /user
## sudo -u spark -g spark -- hdfs dfs -mkdir -p /user/spark/applicationHistory
## sudo -u hive -g hive -- hdfs dfs -mkdir -p /user/hive/warehouse
## sudo -u hive -g hive -- hdfs dfs -chmod o+wt /user/hive/warehouse






################################################

# basic img:

# # docker rm -f -- c7 c7-repo
# # 
# # c7_init () { sed -e 's|^mirrorlist=|#mirrorlist=|g' -e 's|^#baseurl=http://mirror.centos.org/centos|baseurl=https://mirrors.ustc.edu.cn/centos|g' -i.bak -- /etc/yum.repos.d/CentOS-Base.repo && yum install -y -- passwd su sudo openssh* initscripts* net-tools* nc telnet && systemctl enable sshd.service && passwd ; } &&
# # docker run -d --name c7 -- docker.io/centos:7 /usr/sbin/init &&
# # docker exec -ti -- c7 bash -c "$(declare -f c7_init) ; c7_init" ;
# # 
# # docker commit -p -a hm -m 'in -- passwd su sudo openssh* initscripts* net-tools* nc telnet ; enable sshd.service ; passwd' -- c7 harbor.io/vmcontainer/centos7:0.2 &&
# # 
# # 
# # c7-repo_init () { sed -e 's|^mirrorlist=|#mirrorlist=|g' -e 's|^#baseurl=http://mirror.centos.org/centos|baseurl=https://mirrors.ustc.edu.cn/centos|g' -i.bak -- /etc/yum.repos.d/CentOS-Base.repo && yum install -y -- sudo httpd createrepo nc telnet net-tools* && systemctl enable httpd.service ; } &&
# # docker run -d --name c7-repo -- docker.io/centos:7 /usr/sbin/init &&
# # docker exec -- c7-repo bash -c "$(declare -f c7-repo_init) ; c7-repo_init" ;
# # 
# # docker commit -p -a hm -m 'in -- sudo httpd createrepo nc telnet net-tools* ; enable httpd.service' -- c7-repo harbor.io/vmcontainer/centos7-repo:0.2 &&

# # # # basics # # # #



## 自做 repo 镜像

########

## cd ~/cdh632 ;
## 
## docker run -d --name ot-repo -- harbor.io/vmcontainer/centos7-repo:0.2 /usr/sbin/init &&
## 
## docker exec -- ot-repo systemctl enable httpd.service &&
## docker exec -- ot-repo mkdir -p -- /etc/yum.repos.d/cfgmap-repos /var/www/html/cm6.3.1/withdependencies /var/www/html/cdh/6.3.2 &&
## 
## (docker cp -- ./CDH6.3.2/cm6.3.1/RPMS/x86_64/. ot-repo:/var/www/html/cm6.3.1 &&
## docker cp -- ./CDH6.3.2/parcel/. ot-repo:/var/www/html/cdh/6.3.2) &&
## 
## docker exec -- ot-repo bash -c '
##     
##     (cd /var/www/html/cm6.3.1 && createrepo .) &&
##     echo "[cm6-repo-nodeps]
## name=cm6-nodeps
## baseurl=http://localhost/cm6.3.1
## enabled=1
## gpgcheck=0" > /etc/yum.repos.d/cfgmap-repos/cm6.repo &&
##     
##     ln --symbolic -- ./cfgmap-repos/cm6.repo /etc/yum.repos.d/cm6.repo &&
##     
##     (cd /var/www/html/cm6.3.1 && ls --color=no -- cloudera-manager* enterprise-debuginfo* oracle-j2sdk1.8* | xargs -i -P0 -- ln -s -- ../{} withdependencies/{}) &&
##     
##     yumdownloader --destdir=/var/www/html/cm6.3.1/withdependencies --resolve -- cloudera-manager* enterprise-debuginfo* oracle-j2sdk1.8 &&
##     (cd /var/www/html/cm6.3.1/withdependencies && createrepo -pq -- .) ;
##     
##     ln --symbolic -- /etc/yum.repos.d /var/www/html/repofiles &&
##     
##     (ln -s -- ../repofiles/cfgmap-repos/allkeys.asc /var/www/html/cm6.3.1/allkeys.asc &&
##     ln -s -- ../allkeys.asc /var/www/html/cm6.3.1/withdependencies/allkeys.asc) &&
##     
##     (ln -s -- /etc/yum.repos.d/cfgmap-repos/README.md /var/www/html/README.md &&
##     ln -s -- /var/www/html/README.md /README.md) ' ;
## 
## docker exec -ti -- ot-repo bash ;
## 
## # see:
## # systemctl status httpd.service
## # /var/www/html/cm6.3.1/withdependencies
## # /var/www/html/cdh/6.3.2
## # mkdir /var/www/html/defs-on-k8s && vi /var/www/html/defs-on-k8s/demo.yaml
## 
## docker commit -p -a yhm -m 'cm6.3.1 (withdeps, symlinking), cdh6.3.2 , httpd -- cm6.repo README.md allkeys.asc' -- ot-repo harbor.io/vmcontainer/ot-repo:0.2-cdh632-0.1.4 &&
## docker rm -f -- ot-repo ;


####################


## docker run -d --name c7-mdbconnectorcli_ojdk -- harbor.io/vmcontainer/centos7:0.1.2 /usr/sbin/init &&
## cat oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm | docker exec -i -- c7-mdbconnectorcli_ojdk bash -c 'cat - > oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm && ((yum clean all && rpm --rebuilddb && yum makecache) && yum localinstall -y -- oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm && yum install -y -- mariadb mysql-connector*) &> yumin-mdbconnectorcli_ojdk.log' &&
## docker commit -p -a yhm -m 'add mdb: connector cli ; jdk use oracle!!' -- c7-mdbconnectorcli_ojdk harbor.io/vmcontainer/centos7:0.1.2-mdbconnectorcli_ojdk &&
## docker rm -f -- c7-mdbconnectorcli_ojdk &
## docker exec -ti -- c7-mdbconnectorcli_ojdk bash ;
## docker exec -- c7-mdbconnectorcli_ojdk tail -- yumin-mdbconnectorcli_ojdk.log ;
############### not use ##################





## docker run -d --name c7-mdbconnectordl5cli -- harbor.io/vmcontainer/centos7:0.2 /usr/sbin/init &&
## docker exec -- c7-mdbconnectordl5cli bash -c '((yum clean all && rpm --rebuilddb && yum makecache) && yum install -y -- wget mariadb && (mkdir -- /usr/share/java ; cd -- /usr/share/java && wget -O mysql-connector-java-5.1.49.tar.gz -- https://downloads.mysql.com/archives/get/p/3/file/mysql-connector-java-5.1.49.tar.gz && echo e7bc11a55398bad0ea8548163deabaa8 mysql-connector-java-5.1.49.tar.gz | md5sum -c - && tar -xzf mysql-connector-java-5.1.49.tar.gz -C . && ln -s -- ./mysql-connector-java-5.1.49/mysql-connector-java-5.1.49-bin.jar ./mysql-connector-java.jar)) &> yumin-mdbconnectordl5cli.log' &&
## docker commit -p -a yhm -m 'add mdb: cli ; connector5 is dl!!' -- c7-mdbconnectordl5cli harbor.io/vmcontainer/centos7:0.2-mdbconnectordl5cli &&
## docker rm -f -- c7-mdbconnectordl5cli &
## docker exec -ti -- c7-mdbconnectordl5cli bash ;
## docker exec -- c7-mdbconnectordl5cli tail -- yumin-mdbconnectordl5cli.log ;
############### using ##################



## docker run --name mamo-tools -- docker.io/library/alpine:latest sh -c 'sed -i.bak0 s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g /etc/apk/repositories ; apk add bash openssh sshpass expect' &&
## docker commit -p -a yhm -m 'run: alpine:latest apk add bash openssh sshpass expect' -- mamo-tools harbor.io/tools/mamo-tools:alpine-1.0 &&
## docker rm -- mamo-tools ;
## docker run --rm -ti --name mamo-tools-test -- harbor.io/tools/mamo-tools:alpine-1.0 bash ;
############### using ##################




########

# in-using:
# - docker.io/library/mysql:5.7
# - harbor.io/vmcontainer/ot-repo:0.2-cdh632-0.1.4
# - harbor.io/vmcontainer/centos7:0.2-mdbconnectordl5cli
# - harbor.io/tools/mamo-tools:alpine-1.0
# - docker.io/nginx:latest

...

################################################



## meta pod

---
kind: ConfigMap
apiVersion: v1

metadata:
  name: passwordenvs-cfgmap
  namespace: ot-cdh-sts-ns
  
  labels:
    app: ot-cdh-sts
    role: passwordenvs-cfgmap
    release: dev

data:
  
  METADB_PSWD: root-pass&
  PSWD: KOos12#$
...

---
kind: Deployment
apiVersion: apps/v1

metadata:
  name: ot-cdh-sts-metadb
  namespace: ot-cdh-sts-ns

  labels:
    app: ot-cdh-sts
    role: metadb
    release: dev

spec:
  
  selector:
    
    matchLabels:
      app: ot-cdh-sts
      role: metadb-p
      release: dev
  
  replicas: 1
  template:
    
    metadata:
      
      labels:
        app: ot-cdh-sts
        role: metadb-p
        release: dev
    
    spec:
      
      hostname: ot-cdh-sts-metadb
      
      volumes:
        
      - name: mysql-libdir
        hostPath:
          path: /run/ot-cdh-sts/metadb/mysql
      
      containers:
        
      - name: ot-cdh-sts-metadb-p-c
        image: docker.io/library/mysql:5.7 #docker.io/library/mariadb:latest
        
        env:
        - name: MARIADB_ROOT_PASSWORD
          valueFrom: {configMapKeyRef: {name: passwordenvs-cfgmap , key: METADB_PSWD}}
        - name: MYSQL_ROOT_PASSWORD
          valueFrom: {configMapKeyRef: {name: passwordenvs-cfgmap , key: METADB_PSWD}}
        - name: POSTGRES_PASSWORD
          valueFrom: {configMapKeyRef: {name: passwordenvs-cfgmap , key: METADB_PSWD}}
        
        #envFrom: [{configMapRef: {name: passwordenvs-cfgmap}}]
        
        volumeMounts:
        - name: mysql-libdir
          mountPath: /var/lib/mysql
...

---
kind: Service
apiVersion: v1

metadata:
  
  name: ot-cdh-sts-metadb-svc-clusterip
  namespace: ot-cdh-sts-ns
  
  labels:
    app: ot-cdh-sts
    role: metadb-svc-clusterip
    release: dev

spec:
  
  type: ClusterIP
  
  selector:
    app: ot-cdh-sts
    role: metadb-p
    release: dev
  
  ports:
    
  - name: p-3306
    port: 3306
    targetPort: 3306
...

# mysql -uroot -proot-pass\& -hot-cdh-sts-metadb-svc-clusterip.ot-cdh-sts-ns.svc.cluster.local
# mysql -u'root' -p'root-pass&' -h'ot-cdh-sts-metadb-svc-clusterip.ot-cdh-sts-ns.svc.cluster.local'



################################################

## cfgmap-defs/cfgmap-demo.yaml :

---
kind: ConfigMap
apiVersion: v1

metadata:
  name: cdhrepo-cfgmap
  namespace: ot-cdh-sts-ns
  
  labels:
    app: ot-cdh-sts
    role: cdhrepo-cfgmap
    release: dev

data:
  
  cm6.repo: |
    [cm6-localrepo]
    name=cm6-repo
    baseurl=http://ot-cdh-sts-repo-0.ot-cdh-sts-repo-svc-headless.ot-cdh-sts-ns.svc.cluster.local/cm6.3.1/withdependencies
    enabled=1
    gpgcheck=0
  
  README.md: |
    hello !
    
    did you use the demo: http://thehost:theport/repofiles/defs-on-k8s/demo.yaml ?
    
    if you did, you may need these:
    
    - yum.repo need to add: http://ot-cdh-sts-repo-0.ot-cdh-sts-repo-svc-headless.ot-cdh-sts-ns.svc.cluster.local:80/repofiles/cm6.repo
    - repo url: http://ot-cdh-sts-repo-0.ot-cdh-sts-repo-svc-headless.ot-cdh-sts-ns.svc.cluster.local:80/cm6.3.1/withdependencies
    - parcel url: http://ot-cdh-sts-repo-0.ot-cdh-sts-repo-svc-headless.ot-cdh-sts-ns.svc.cluster.local:80/cdh/6.3.2
    - metadb: ot-cdh-sts-metadb-svc-clusterip.ot-cdh-sts-ns.svc.cluster.local:3306 (root pass see: passwordenvs-cfgmap)
    
    first use these? you may need to check the first installs by tail ~/mngr_in_retring.log , and see scm metadb init by cat ~/scm_prepare_database.log ;
    and these two commands is usese in host ot-cdh-sts-organizer-0 (or ot-cdh-sts-organizer-0.ot-cdh-sts-organizer-svc-headless.ot-cdh-sts-ns.svc.cluster.local)
    
    testing cmd:
    
    - mysql -h ot-cdh-sts-metadb-svc-clusterip.ot-cdh-sts-ns.svc.cluster.local -uroot -proot-pass\&
    - less /var/log/cloudera-scm-server/cloudera-scm-server.log
    - systemctl status cloudera-scm-server
    - jdbc: https://downloads.mysql.com/archives/get/p/3/file/mysql-connector-java-5.1.49.tar.gz ; md5: e7bc11a55398bad0ea8548163deabaa8 ; GPG: https://downloads.mysql.com/archives/gpg/?file=mysql-connector-java-5.1.49.tar.gz&p=3
    
    hosts:
    
    - org: ot-cdh-sts-organizer-0.ot-cdh-sts-organizer-svc-headless.ot-cdh-sts-ns.svc.cluster.local
    - dcr: ot-cdh-sts-dancer-[0-2].ot-cdh-sts-dancer-svc-headless.ot-cdh-sts-ns.svc.cluster.local
    
    you can also edit configMap : cdhrepo-cfgmap, in ns : ot-cdh-sts-ns, to chg the cm6.repo file.
  
  allkeys.asc: |
    -----BEGIN PGP SIGNED MESSAGE-----
    Hash: SHA512

    - -----BEGIN PGP PUBLIC KEY BLOCK-----
    Version: GnuPG v2.0.22 (GNU/Linux)

    mQGiBEnvgi0RBADLx1qQlXlrvHOo13dUvoWL97Ny/0s0S/GcMEgAqYvZzUPVcq8H
    GUsOb4PLTfcL1H7Ptq9fqr02uIb5Bc/ltdwE9GFaT2nvdfBx9T8jr8LrW9JE2xJq
    dCyFO5yP9YbZeFAxNO3yBxeP85lQ9CdWWLvyYdtQ+T84EYerqkcVbSvYRwCg6zyx
    EE3jWYvyVv/3HTrVTYpgHgMD/2kMR1Z2vEYOSM7h4cnRnxiadhefqJ2WCm4L30Rx
    /F9JBLAEuIuUndiOShoB043iDY+rrqCHqHQ/uI2D4piW9cDYMo7EJlsFtQ5g2SFg
    PcS4+DLhU464dTQsTGAhvcv+F0VQV4iu1HdD2/kKJkCS/MZL4rr4emqsh6VIBDdG
    ytPaA/9cyRJZe2BrBM2pECGncE5RUaM3g37Ka+VnmMVOXgZdzgCxwFZyVhyxzssD
    kB4jcm75UEZx8BiaoPQDQEsBongdx5M4Vwv5XnvUq7sK7eZLmUzW9hmkPjgLea0/
    znchvPsLeTNqSfIcH14TbFt6B2y1G3Vbi5/6UiAaIqLrqjZlCrQXQ2xvdWRlcmEg
    QXB0IFJlcG9zaXRvcnmIYAQTEQIAIAUCSe+CLQIbAwYLCQgHAwIEFQIIAwQWAgMB
    Ah4BAheAAAoJEDJ1dO4CqBjdGQUAnitydC/NGEh0aZXDN1v22pWFpRzTAJ46N4gT
    Zx25oWfyppX3R7fSH+7TPrkCDQRJ74ItEAgAq8s4iMsGhk9nnMF6wlarqHjws4Dw
    NFZBzA1Ah8KnMtrdr8t99OfzY1b7PNzHXujcaTTqL6L881ezdsls9aHp2kr24Btr
    8nqEZJHSjCnQscAGu+NrhoH2KvK+tMRCHGRcy5UNQbLTJi4Hf8Lo+zv0WUy9BCDu
    7HoDlwrrh1Rw5oOwLFc2UXSTEB6BwYna0mZcNjVpfKNHa//wJcKR0AtsCwRT9znP
    GS0Hpqi1l0/iU7sJhNWyyF427ANg+Jv2n4IP+dd734ZiFeJ9tWCtBjfc3MZJKETk
    tiCtX7FVIIqBAmYLwPqcvZMGJMrNzLBtRuuiBv5bFcPpMEhoD40oQEG8uwADBQf/
    f3NpQbuAcZLMzbrHYu3FB/+4ETvDJXJIEUiQUdobWancSBUhuNPOqIgLzIWM1jRu
    jWGIpkeP6iqNW9kDrq26CuMFP2CoVvnaMiLXUvyf62HWAiYXXlZle5O97bvhYMtM
    Y4o5sMo2ktI9IcgYIFicFwcmuGyGL7nJ3Bo9FAUV2LvMe++O/f13jsPpygoTZgGT
    6w0erglWgrgf5pXt8ajlI4TUrlMVg9Iy/tB9ZzVHnpk21o4vLHwZkgXe1WlK/Rze
    ZCruXyXHaFyEJN2zlP2xNj2F2WisL+/HEnl/qzU4IpNI2LQV2aiY9Nt8MBXgSHAh
    gWKWkjiB+tswgzuNsBOTM4hJBBgRAgAJBQJJ74ItAhsMAAoJEDJ1dO4CqBjd988A
    oJ1WlEx2BcFA7W1RMyErejcvB6thAKCf3t0thSQvkoGi3AOJ4Haj/C3yUZkBogRK
    QYBFEQQAvgi9WugX1RKCgF9SnDh6qvWddFnTxjoEzPgLZzlTyXSbyGXA5GBlJsnK
    WhsWN/sV9cfGb8TW+1l4novBTD5uLEf8UAvwhdokud/fbYyyanAxI3ak1f8IUOEa
    4NSSxmh1I0dGJqzrXk+9cYpBMJQmpU6MULoQc+zc+GgXgwz3/ZcAoLxvsQmjsYnL
    fh3tN4OBzUk2dE9FBACMSiE6QgOmb1D6BBoaZ/hrfsCSv0nAdBSgn1vALkEGT+rY
    MQxING2mrE/DMbl1xNvqKWmlj01WuuMLOzr28h+c9cYQTseKkVQcfG2bBWEFJP9T
    wb4OsgOEZmqou5006B2rT+vfyTC308h/LFxEzEPM+U9CwA+muRCwGAaqpWyVkQP+
    JZG2hiMJMi3DgKKf4NEghER7eujC9acHY8b8xNq0ShVG41G0WbB06lauRAYKXEyH
    k7kctNskPgYfiWgbWIZ9AGOfre43AnSXk2oUv+0SiT2D4kn24COs8KRzPZF+BU/t
    Mg4J1RdR/4fadfVL/92oSILe6KfzNXgHR9P3j+g79OK0J1l1bSBNYWludGFpbmVy
    IDx3ZWJtYXN0ZXJAY2xvdWRlcmEuY29tPohgBBMRAgAgBQJKQYBFAhsDBgsJCAcD
    AgQVAggDBBYCAwECHgECF4AACgkQ+QwNj+j4as0aNQCgiBzViUnezoqW2aqefH/w
    VmiAZQwAoIDUR+B74Pdrd8bcGBXaIUVhdl24uQINBEpBgEcQCADGjzxn0L7C5Mi8
    jN3j7ZJB4tG3sNFxltQmMVjmSzLvMJyYDB6Xzw9nS4KejxIfs6YtkjNYnkBgFuit
    ONE0rgvT/i3GQe0sWpyBO5TjR48+7KRkIn7d9FhWXGiIBfjLsmSIoTYe77WfYqfC
    WWDwPPF0TVNQirl8eK/IggyeCApLBSnjsEFLs3/yESWqcxGdIsX1U8MfdchtdE7x
    yutwUS12BwTC+DoPRWj2YBatjfWM+Fqbvz5+rY2QDHkiV5tqjTtj0qEwv5Gqbm6T
    MEpjgZWaqx4vZXCFq4nFxPXXMmhlUQH9wiUZV+GTbsHEI8pLOQRYm1N/86pcz+io
    ME4A5mmjAAMFB/9OCkTN5OjsNipA7dgSlr3jiNuJpCEGoVCe/4mZSrC9u91XBkGe
    41NvBk9BR/YmHgIG6S66xLgwBWaRpvUaDlSRp8Vl9w1S9fqpIkueqV31PvGrWZZA
    hsdPSIpSYH9AE5ZmMW07WJYk0TyuJUGDnzQdb9t23ZQcCEo3A0ZFrttmqGrUgc6/
    xQES+n3WWaqdqn1jpnLXoi2oIqKOrNKIMSjUB+T5XVEMvZ9lv6Dl7LObApZ1Db8y
    pc88f6RZE/GVAL7i2NL/IMpJOfOr244rj5IPLgDgYbZ7ElsCepxD2kQp7osJahsY
    F5pye1DMzoppLn9F76mr4ZDBBAKkSZDS8llbiEkEGBECAAkFAkpBgEcCGwwACgkQ
    +QwNj+j4as1FdgCdHldwuNO2N14uHdu3JS7M/8FtBzIAoIXMQpojk9pFh+mGu3fZ
    5Cs9lDPmmQENBFbGIf4BCAC1sikg19e42rYG9BKy2i/RWw2vlee2MrbgvvJN8jRY
    E6QGQchxmiGWaH97UEHWWSvZ0bHld7c+2Km83Wa71NxsfGUfI837gmQbM3BYEtGW
    fgTNOiBJrV6vSfSF64lk8VPdYdbyy3nPWaNm+qMQk5T0iGyOUz3a9+Aqo/4x87Wp
    Ay1k55qAfxgY1STcgKhw5MQ30eXJLsNvRUf3m/SiiXwLEc0EbU9ZesrJDZ/DPjOz
    BEwW4YPJS8uPWczmd4e/uNxi/ZAEiGMaFBOL88Y2V9NRxAgpGBKPdu3+FaC1Xvtw
    +gmPHjRf25irP2UtQdIqkMAyVn9yLmDcyeC6FBVlZPPFABEBAAG0K1BhcmFtZXRl
    cml6ZWQgQnVpbGQgPHNlY3VyaXR5QGNsb3VkZXJhLmNvbT6JATkEEwECACMFAlsW
    +osCGwMHCwkIBwMCAQYVCAIJCgsEFgIDAQIeAQIXgAAKCRBzmF1DsLGcn1JmCACo
    L27DZEyL/MFHeeKFA5M4SVZ4+qOsg4Lua8kn7wv6YqscRwhjx/HE/6DxEtskpPQj
    Uqml5a1jgUodiS6N7PPF+kB+Lf/D1R+lqRf9uux/efu+VvJOlYcpjvEB0yQuXgHl
    R59gzrPJvBZexE9xV0dvzzUKTxA6jmOkaIt1qKv0Y7BrZAMzB97RWMyW8H/Nbx0L
    xMYhnN6pwhb5b993MTEecv6y61d4e6qNeOKavkNUhiWL9r4NZgdqM11v3GZ1C6VS
    NYYKwohH35dtCuTYB2Phf5QTXoiWFKqGJKiJi0vHBukV1+6Y2WdgdnKKjiz9/Foe
    VCdUueWDz21yAQ9vyldluQENBFbGIf4BCADBWH7uZqLHP+s5mxyGFGvIkeohax1b
    D2sjQh6m+rvjXVfn9CjQofjyAI1q+FCTvcOevC60y1SIitKAPHNhEI22lh0k9jh0
    ORb5pWms3pe36H8+2aBLrOj4qVyRRIFXhbLAUZXenhP+nH0yBMqk5LH3cXOQmZZE
    JgMkQg+qFt7DLVwR3wse2FFYeL12CEYSKG/4bfQHp9w9MErufKiFGPF/e4lHOVTc
    7Q2ynxwkGsfNqEZxD+DJWHnsd0GY+sjeLKaj+EaWJIzw1+KSsVw2GATKx0u7LD0r
    1jTkqYCjoMVPEvAowMeaVYK4QbLysQJlHJLB7IYq2HYtL0ypkFvVdLgBABEBAAGJ
    AR8EGAECAAkFAlbGIf4CGwwACgkQc5hdQ7CxnJ+juwf9FT9PgfIc5c130eQOnNuO
    pVPF7p1Uc3YJLuwPf60qPQqzk8ZD0pX2BM5DmI8yzmybn/C+PuNRWEPbuaS5SqGY
    Cn6EYj1QFlbt0iqt4DgP2/ntWz7mdLgWfn7/h8ruOoITF2vRc0d9Vpf3XMXYB1Fy
    9LbnArxLCJwj60QMsI8cJ0Eajj0wUfMAXztj7gcm+hTMGMh5GMoOZ+7bUcr3HTSP
    pXKUeAq6ATEzZ++0ZwAYjCC2bGFjDDQf8R2EdYwF2fZQ6J4t+E46YZo91X4BqM88
    mCDKzkYD4KxEX8xwAhW6dAjyCyYXA4Ejo/rdLWBfe0mcOpWbKAINY2Bv+SavbLUM
    VJkCDQRbMWl5ARAAvEcGks9NJvqX6DOFTSfeazOUqTrmsAQsk4ZqnXvt41CIITVB
    bXa4b27LPenRERU18ZE5dWSe5VDpKjhMoIvhkYg8cgMpiOeqGG+TviErfssFV5V2
    S9BIV4r58R/n+zHOHa+RQIweu8FDa+F1yIJ0N9oPfvGeqyRWd2cQiYL4/1qlaVy6
    ZFNmGabTDiR7ebSoZzTAvZKYyjKL12MuWG3/khYS5nLCanTtzA/JO2xhYWjqJN/9
    SgqRbQKXOEHoz1M3vG26XMNqQtWraHylCeU0X5l+QtwrpnG5Mr1NYJYPo2oPoCGG
    tKozWSZne71du5wbdoUhMtcKchJ6QLy8vBpc7M6eZVNq8WD2GX9U9zo83TOiYpNI
    ZBpU3QP1fD5P8Ltr+aD7QTHugJPgHly5KX1+OmJfn3GGd5azSUp77PMuZ8mTu60L
    b8eiUIcrzji0NxGg8+wUPasWT/2IlAZHP9OhW3Z1UxODcFovVaH1JIgcXq77LYN7
    xl6RuRwqU2NKFq1tunSg/yg+eBZ+OTssbo4YHqg6lCgtdnVSpQDyi2TfBduAaxOf
    Ofc22mu0hgHaBrzeE+wzj741OFHVXO/edsXa3QGi8UROMTARsy/MZKl3bMLfDTZC
    0EO28iiX0jGUTuYuGPCaR4Q4jpPsG6RChvAsrnoZ8ZK5cu7dvpOYNmgCc6EAEQEA
    AbQgQ2xvdWRlcmEgPHNlY3VyaXR5QGNsb3VkZXJhLmNvbT6JAjkEEwECACMFAlsx
    aXkCGwMHCwkIBwMCAQYVCAIJCgsEFgIDAQIeAQIXgAAKCRAieWYnhEFXANpHEACH
    JKWul0Lv3nkKZ1uoXoNUO+3A3iUC6e7VD0YQN60MB5so3HJczzs7bIUWVzl4X4tn
    stemoZ9sz7B24IJBvy4rWKO6459whB4rZQBYw2v1lVklBYwZPgplXTWpKmWJacn+
    pxBExchHKuT4taBaxmF2Am6HNiEP2rBQTQa0ZiXoagS6XbTBEGDYfUgPop4GHh6r
    pMWTW1qzEpWiTSFVXbkzpfQLCjNhny00fwODXIH5K/+15IQAJfmzPknK+ajmrM6z
    jK7jRxd0+seAlsCqaedV10wEHDIqS+T0UkZbVbNEVEBbTXV3c/MOjVnDOKqSbnlg
    lhC+iQBh8St2aSHiQTU998DvOwpOrSnzMKXhZ5vS6AWi6co5YLqvlK5PFYp3VlUX
    1RwmbJMS8WYgjKg4qPGdprYzHc0L3ySoL6sXy6mjoXFjKd6LOm0dnFBSyeMaGG31
    1Ro1dMGNa0iEeWuYuOj/gog+fdeTliVe9uQLeOBxdbhE164r/H513ywuE1ENVVGu
    /Hc8eddmr4tcyimUt8E8h4G477lQ1CgTx4G1k3TH0lY0Ugwh2iHmPcDycURN8e3I
    d9jY5FYCOfXbiYRYr3iWy7JwzA84gGzLmhRgVibcw0dGSMEAEvVhi8YgScQJ0B5+
    8ZJPsnFfw4SEsFupf7PtYpsdoYQ0ekeUB4IxF/BScbkCDQRbMWl5ARAAvpREob7o
    t9uGwGOgbh4PguZwR9RgtjOOF9DYiBS3d5Sji7gCZFa5+USp0egAJAgZZBcfVmem
    ja6mp5wKimHVyyRF6kAb2v9bLNPUiuq5LNqi+gR7qO/BmoYYrEnvM19XBHSu8KEm
    pndLO4F+zimVgpGr23HWQho1qh5DLkmRkfmZwba0RRon1hfl7+W6hYtgVGgHStWZ
    oBL0yemBOpGjD54/TPsEnwA/1dxEWI4Dwt4wbfsnisjSZFsJ8E1Dl0pWHv1eofgM
    qgtg+gFM1piUGZ3DpX0u6Yqpm+r9u1GQvedH4Kq+IsHzJDzfrk+jn5iZQSez1JKt
    YVkOTMuBe0FyoXslGnEtTSvA8sZYo7f6NixvjLtLWWxkuBUpyMeztEoMD7OmTT+1
    RLFhiw5MwpQjNKyPXYe3/OZsGzh1cv9tMLCQBQHJgqfxAwOxlYO/5mS0yFJw2Lbq
    F78/adC9CrWTUafMYcQxK3yn+Lxrf7w90sJdyd99kppAC5zvUcHj74i6DiIc39RP
    zxkXSG1zulB6Tw/r7QI/IF2z2+qI29O6Y/oyhLmTIkds/uQu3rPSDOw2mOylkQFE
    XW7vY/RtyrDxwVE9VX27PJeBJvqwoen+32dFQxMXxPp72C1+gcDuE5U6yhuXW4LJ
    PA0up8P6qGounJmBySei/nTo9RHr0HzjRl0AEQEAAYkCHwQYAQIACQUCWzFpeQIb
    DAAKCRAieWYnhEFXAHSCD/9cogh5r3ysRCvvw2TRNhDAt+ND3Ra66OeJUJcR1CW3
    9GH2k1q59VEQmAqpVD7x6xY7k08XL+17YPhDalKTrFTbrHbqXpYnHc8CQatpmCVY
    jNcewYm1Zd3qvugrudQOcRyS2WhGg67zUJuq8SgJe0zAfSM/VsEaiAgLFFDPM13w
    4m6scSxm19jNk5za3uDUYKSnA/cjJKzTGJK2mTJEB0PMGSorCbOpSJo+y22uRQJS
    QRAdBN63CR6BWKK4113QTrU61m3wdqXupIT5F3kqJJ4EkdEGr74JxZC+oDxpvsCE
    I2ms/+TdPG1EOnlDAfis+JhHOgpQf+E+Ng0Ai4tTLD1ma2aHfFKSnvza3ufbgYL5
    Qxwp0oSRXs1IjtiSUctUDAWbiiuUp3AKl443TAdl2zidYcZsPp869yWvKSW52+te
    D1rsj4RrLb18Vq2Tm3OTN9T2PD/BApmToQp0mW6fr60EvNruU45DgZN/LrG8KgVC
    8zieN+vEgnTF5q3T/NJPgYhrL+cuR1Z89CgprcIiJUoOovTuP878Kgx8/gyeqglk
    6qVd6uv7Lre+aWJfYL7AfJE0HR2nLYLZ3IK5moh9u0OFJ6iWuvSqs+comG4ijbZO
    ptrP/a6m11fHzs87nUcd7HzddcasPQouvR163hQe3hz/Ym67aTPOcOjWys2+zwdj
    JZkCDQRaVSyCARAA0v9zZcc3KLyppFw4wBXKDGjiHtIE/T5u/m1+VHxCOV+0yy3e
    eNk4crARcaRCAOIXAszIq2nph/PItSkopTGKqbLSrt1X1R/ZDM9jfxHG/SJ1Oijp
    hlZvuUYZcYSqIIghaaai8NcecSNvRN2onyhbF7FVJrgYx+cmK1BA+tcZyn5u5TZ+
    5iIyYVVR1GJmcxId5Rby/cr0CF2TmgW7Allmon4BADr+J/HIozQCnvz8OXVhXQ0B
    gJxp0pMr0KpoJWBcEU2zT68fCUUVQBtL/Tw2wkkQC0yP3IPADVt8tgMvGGPKYlri
    XG4lzNyLUmjCTpB2/UPrOQ8gvAxwh+2UZFLEaIZtjboQ4dpJCMzw4K7/K703n0fw
    SEvlEj3MYlvoNv2OHNaQ6s4tQHZGZKRkh369OaEIjGbQMgLwvTzQ5Ct+2JFnNW1R
    r7agk+l8+1MTwz5jLewzQQf9E3dqZUUEzYdouyuYhf79s+g3C95/sAwvNXeiSJoG
    9rr5s/V/13lvKqBTzKgG20PxHEX2t0TVnVziWhL2MnDbvKavtHTMEpaIdzdpbw9k
    fbZPIgTguL1D3d4kBQrXP4PPiJk6473Xxuibo5TaY0sCHff1FNEYSzMj6JcpThgC
    dZkx7nj7ao08QQu0rN9KyrGnn3st5fy+kUGb8uyr/eSU+Po7v0L+Bb7MfGsAEQEA
    AbQgQ2xvdWRlcmEgPHNlY3VyaXR5QGNsb3VkZXJhLmNvbT6JAk4EEwEIADgWIQTf
    LE3XYpsawIoJZuAPZVUnNvV/NQUCWlUsggIbAwULCQgHAgYVCgkICwIEFgIDAQIe
    AQIXgAAKCRAPZVUnNvV/NUekD/9XPUb6IQtKf9YUt7pkWP0QVYMmJpcdCUTnBf9s
    NALcl6frwcJMMNlJwZJ8Loq7FB/VkkPy1+TeeOrgzWSSZLSSpGZr1lG19V7fVeVL
    NuBEl6pmrBG4vuHrHb6sufFn2JaeTIu01VJjtFpiY/0yVDejwLvna5Ma1ELVsEc/
    JIDxhe4/c9pQtTZMReiqrOvnrUfI23zxZPBXjeKocdtRV9jQeXjM+7Wldg9A0KuG
    YqVF+FU/WphazTYWYJY0seAKh8WuhPH5B1rDXZQ4TOb5WpI8RYvOFZVqJkRYPW+n
    K7sj7Basmoo78l9hCJN2RZrUW3g1tjlRQ6ADcke+TvuZq9Lrhs7pAcxFTe/I2Ubx
    4UFCzX5RiCoEVbvskCtI5iXDvMVtHStPSYj3rxLO+KovGNeouuMrlCIM3KlHSPUi
    VdE4AEfTAEBYjb8KPOEOOIybM2y02SYQ4p1SHDpMNnb3nPuSyq+W5FzfbVrhY3Cq
    XlchEEQesp4zbUKyv6wtZnikZrpIlk65z4lm4Bq9kKA2/csR9ZUli1JHaD+X5Zco
    I4IcGFhgOoaRChch56mPCt6NBf8agaGxgeFpe3COAC18JLxT6HrOe7PGLdu7DBz+
    VNHanK+nlprCgAienED7pB3mf6xkwyr9NYRex1bZ94i+cXk/P0Bd+wduKj9PpiiP
    NGqaf7kCDQRaVSyCARAAwnWKu3h9OlErIY1NKD5cWJAtHbP06nhu5j1xXirnEiZs
    8vGChemlRCJwAPELIA1F/maitmm9HQT5mF6j1A6R/JZiW1S225jm62IuT0nM6lv5
    UgeB94d0FUmEYI5cf9XtsKXs9abYgnFDj+zkkqtESDoVhN7WHalCEP7a0a/noPfe
    9scnxbPcqAcKHa6HGqP8bzLvFOm54rBGtr+T4zidCM8NkS6btLL2utjbfF9M7ubk
    zE6lJrBMzsyeEwda+eZB6dSh2ZGiCuidLzyCtQKSuX2uG6aO4+TbOU3OiBD+kBdz
    IFCeGypBdFaMIT9Pas2xHATjeSCr9iTpOVxT2ZrmAqZD1M5tTqQhQ0fQ5baz4RIe
    hSew2m2VXhwjBEBqzsAnaRMJsvagicvJ7dKfiFDgFRexQ+vpxdQU/pP7eTAg30Ix
    aOKhjSah1T+yvOVeWOSfTlmTYr6l3/BDL1kX3NVQWC0Dr9dp20fzWtITrtAkmScz
    epCu+kqp/arOaGGd72AGyqCJpPPQ85+iR2uU8QPnU4CQUuOVWDkkdoI16otUuNU5
    BW2El6+VOAVMbKxu7RutGoaIraVArmDxePy3UijLGWlo+xmoAz3B86jW/k0Nw0rE
    FVVwm7sVymM/BaYsJ8VZTmZrgtfNxym9FxlRBxyvU+Xwv6BU6GU9YNwX5PNaNxMA
    EQEAAYkCNgQYAQgAIBYhBN8sTddimxrAiglm4A9lVSc29X81BQJaVSyCAhsMAAoJ
    EA9lVSc29X81b34P/jQy+MpLLB19Te1ufL86Ar0SthxVPyJ1aWvYRcf9DhD0s/5L
    XozptH1JSNz4rJ549G8x76jgzjtF1PAkemrYzarPIveIUcgoxzFWhVVuEWZIyfwl
    UESqD8LXdyv8u5b5f6W8FIRHu0aWIPAl2ZHtSDTJw/Cou9ux5NqjTFI/EwAKuPnH
    SNKd49cuZV3tOuJF/qKJukzyghGtu198b25nfN2YBtfeYXk/7883pEju+tf6IcJp
    aJzE52NKIVUYelO1n9k4rW+rR8pcJF1lg1CwfTs+Qc6eEi0IQCJwwu8S5t5SXSi6
    koBjA0YcgRM6S2xdBdWI7FlUUntORYop0AO3LD2d5QrnjyC8YqB5p9LHMqKM+laP
    u3n4CAsJrWc/l506xlhLAYxnGD+GkFcgCBdvuZ7yoZSKAPHoPy0CMlsOxUwmKdrD
    ldloUKpwTbH0jyTa8CDBk+TVt8+f0ftHEFG63AKgsn3PpkJ7uM6bG3urpWBqvu8Z
    iY8ZGet8HYFdz+c7xV/vwIW8SYhzmvFsvYuoWxsCCgpwdTA8PpxhUbaRga157tTH
    KovBD3t6gdvMIVrCrYZ+5sEwpV4tdvz5isRgua0OwU2C4XyNYT0E0wZmUkzOvcRj
    Xkwq61d0me3wMu/IA9EnGhStYoPHLwP+wLD+c9WrO/SlXA+aHd137X+WqV50uQIN
    BFpVLPgBEAC44lEttMjiLRykD7fABmK/jAO9dlo8xuQcqJrYAIza62frrDiHeT4J
    r3kwCk0DEIJvQQFGL0OGSWHCxpYMcFcpM3sHQWQ9wtSNr49qzQpHWfa0EF/4duFw
    Po4x+NLV3Dx0dYK40aIl40Pne/uSNFWHb7jYw4Ohw18lomFX0GWfJlXsHSHfeBq1
    vyn29PRlYM2IJqYRrq8WWeN/Sabx24VNJbsgmuERI79wP6LWGME9PDppI4NpSv7n
    nggvDXzAzqTTLw9wV+fHL1MpsnxKlXLD7r1b8dKW9w4o0/GXtNhk54LSQ58DxVyP
    EyLFtnYW+cM8cbOtoYmqLP+3MCrVhfH61R7BNzZeMJetegsGQV+DQop2Jh7LaZWb
    N6irsq0n+WQDXLf11ShJhX3dnghaIoXGYZpetWUV2vgzj+oeh0LP6+nWk8SLfhVD
    sMTysEif4dyG9VNhPLTjP6mLOyafu6HQHHmJlmYnAX+xXoGwlRuWRqwwbjWW0X27
    r0YM5LR6oQuf2KLwxqxhjRWx1a21H78TWy8GmnMut7SvH1HOiqZ3l+o7+E8X3gpR
    lgPJAp7JveLkdUi5gbjAxjQv0x+tZb0JeoAeB4FIlE2/qsEnxK9Y0JimC3ixYqmt
    Qglm+cZvN2zQfjeHOXXqjcsN9wx58UeeXZMkbZguPr8hVu4k3tX1xwARAQABiQRs
    BBgBCAAgFiEE3yxN12KbGsCKCWbgD2VVJzb1fzUFAlpVLPgCGwICQAkQD2VVJzb1
    fzXBdCAEGQEIAB0WIQT6kjVCma8PXDBWri/X7FBQpZh1rwUCWlUs+AAKCRDX7FBQ
    pZh1r7VXEAC1gvKQY6SDwSTEpIWgyChk3SJGJ6QGupBfUaDWER9ExveH+fRbXtLu
    9YqmzUNqJDszcb15ERfTr6/M9U3PTTExs6navDN863KbMkNDNt8OoeZCYnfCBlDm
    QfkfUpnywbo9yBJ45LZH1qxG6sIFDJX7vGgPVX8PjITCoJ8a+eZgAwqfR/PkyN0n
    lgJToGdYklylLiy1GNd1jXY3X/7HPgPeFVk5OKUwzGDJq4wLiD3xI5H73ScAVdwp
    kJj/rdvT7G/d9F/JLbwu5bRHqjnw+uzt39696w37z/d+BiFMYQAIT4t9rFoAltPR
    PdE0o92CIsYMhHHU9vqa9iiGhw4jGif5ZLoAlL9BZyXO6XS/iR+wXT6k095d6Kg1
    egAdXGnR3Pq23mUbn46rEnw8PO7B2KFYDyQyPWiNGbv/XJ2hJK50G0sbdJwqMHEC
    pclJ03h1kMQ/X2JUp/DoDfymILq1L/hngPeqV6cHvnWNKXeKRXoogtbSg48yyloj
    s6dTK5IlgF9u6y6QjbaiiHVL77wiFijN4fRDFInJNSjBQVfgDptFLcoxVS3ArlkQ
    66b7TEZTFA/eaoe0lu5I5uroSIwsMp7q2dpAl3tmvr+FRZeoY5WV+xy5FvEjdx03
    G0yIwnHvh1/3kpZ/aeE4QuidE31V5FXDAbbXSzY/iNZehW8nhFi714ddD/9jU7uj
    JSMa8lDb5ycPgEwHlucwPnech+UvUK1bgBmPSR8D0nKY83RdP6PvTe1utNE8PhGx
    cpsRrIFJuls0T9W4GNrqDnvZLhlQq/jN0DKxszzz91sZYRLAON98Hr79f1B6fAhY
    8XnG4PloqxFG8xmQXfCVyqkal8tHI7jF047KXA3cVch4pB7CRIhSmxAmgBjQtHbA
    fHm9+q0E8RLRFrRCQXvUYlgA2rPXVL2Y+44MoAaqt7Sw6B/mBEwKkUNb9HXBVA0K
    88RDviL5/p5nVl7mFOPdmci24vRfbRrH+AhWNWkhEyA1JZoTNiioruSc3UBfaK4/
    qpKyc62V73/cKz8/f9jlW5tDoVB2Xrv6ZRxIyvcZvNIemWnxzbn6iMOA4eW/4Txr
    jVxEM9xpWSPKL3GWia4Qq/Pp0DsqeH54i1ytbqax0o4yis4ctHFySwzyJO9PEcba
    tRC6owCi2DqwPN6cy1wGCK/Lh3rbDGNUk7t9KmdufdphsT/3WhtDUpe1QPiAwCCe
    l/tP8k7AfVcxa9RMT97KLLs1wxYaGkzsCBh5QpEFjCw6MMeiWvQSYW9wvsCzxntx
    EpbvSUWyuc2/gHTbcG2EUrzbSb2staAQFDZRqMI2oeK84thbAp1B45kvhPjzRAqf
    NKh/IojDgrdH/L+FBSzRRn9YHyv83CtdRwUSzw==
    =RnX5
    - -----END PGP PUBLIC KEY BLOCK-----
    -----BEGIN PGP SIGNATURE-----

    iQIzBAEBCgAdFiEE+pI1QpmvD1wwVq4v1+xQUKWYda8FAlsxha4ACgkQ1+xQUKWY
    da8AlQ/+KtEvABoGqYVmVixmoGqYuNJ3V7MeQRIYvbvQyXGBOU5VwNAJO9zlscOe
    t2mv0ubRz25K+vZqlrrAiCzDWrOzQnl7xJna8EIFS76ilsU/WaF5DTk+svtLgvs+
    fr5GmbPHLnNTEh/nBe8jb9akxk77W/7ceRfWXS1OYfdwmW0bzasYIWJjXKZ8ONjw
    JN/DcIva3n1F1LeN7KTLi12IowjHQjyRgh7wdlDPbXgPHHvqYy5ILe/aXpX/Qwh8
    A7qb+U5lLcCePZcyFJFoGCT3ZPDTQzMZM+lucizIhKGDH+QjOd/15Q2dw2QnWyLb
    vzm3Lf9wtSda+d2HTrkoD2ksIgpPPdFKmOgYLto56Os8mlzkgQJ+a3OlqWeajJ9/
    IqivdX5J4T8kLdwGePUAoqvR0sKGSDoWsUIOC3ONGOEixidisdnewEHahRRIxH68
    //0OaZ996ga+7xYe4fkEEplpLaNFvOpPudoxsisvBRVLEYpYJ467ETqIP+ChKJRy
    KkCjGwwds4Y5h1Z5DxqXjwRWQ/sSOjM0qpYZCGZ+GV2JFSeEjYps5T89IR68lo5V
    NGCEE7g0CuFZgQayPEiOZCcOCiRwpfC39zn+lK51g2xUj20C2KywjWNamtJQI9Fk
    k3qjiuTU3Z4TYkR55cKiDy4C5aIEbB3Q84bdsudq4ps/HJQ2aPE=
    =h+J5
    -----END PGP SIGNATURE-----
...


---
kind: StatefulSet
apiVersion: apps/v1

metadata:
  
  name: ot-cdh-sts-repo
  namespace: ot-cdh-sts-ns
  
  labels:
    app: ot-cdh-sts
    role: repo
    release: dev

spec:
  
  serviceName: ot-cdh-sts-repo-svc-headless
  replicas: 1
  
  selector:
    
    matchLabels:
      app: ot-cdh-sts
      role: repo-p
      release: dev
  
  template:
    
    metadata:
      
      labels:
        app: ot-cdh-sts
        role: repo-p
        release: dev
    
    spec:
      
      volumes:
        
      - name: cfgmap-repos
        configMap: {name: cdhrepo-cfgmap, defaultMode: 0666}
      
      containers:
        
      - name: ot-cdh-sts-repo-p-c
        
        image: harbor.io/vmcontainer/ot-repo:0.2-cdh632-0.1.4
        command: [/usr/sbin/init]
        
        ports: [{name: p80, containerPort: 80}]
        volumeMounts: [{name: cfgmap-repos, mountPath: /etc/yum.repos.d/cfgmap-repos}]
...

---
kind: Service
apiVersion: v1

metadata:
  
  name: ot-cdh-sts-repo-svc-headless
  namespace: ot-cdh-sts-ns
  
  labels:
    app: ot-cdh-sts
    role: repo-svc-headless
    release: dev

spec:
  
  clusterIP: None
  
  selector:
    app: ot-cdh-sts
    role: repo-p
    release: dev
  
  ports: [{name: hp80, port: 80, targetPort: 80}]
...

---
kind: Service
apiVersion: v1

metadata:
  
  name: ot-cdh-sts-repo-svc-nodeport
  namespace: ot-cdh-sts-ns
  
  labels:
    app: ot-cdh-sts
    role: repo-svc-nodeport
    release: dev

spec:
  
  type: NodePort
  
  selector:
    app: ot-cdh-sts
    role: repo-p
    release: dev
  
  ports: [{name: hp80, port: 80, targetPort: 80, nodePort: 32008}]
...










####################################


## PVs


---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: ot-cdh-sts-dancer-shr-pv0
  labels:
    release: stable
spec:
  capacity:
    storage: 1Gi
  accessModes: [ReadWriteOnce]
  persistentVolumeReclaimPolicy: Recycle
  hostPath:
    path: /run/ot-cdh-sts/dancer/pv0/shr
  storageClassName: ot-cdh-sts-dancer-shr-sc
...
---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: ot-cdh-sts-dancer-run-pv0
  labels:
    release: stable
spec:
  capacity:
    storage: 2Gi
  accessModes: [ReadWriteOnce]
  persistentVolumeReclaimPolicy: Recycle
  hostPath:
    path: /run/ot-cdh-sts/dancer/pv0/.run
  storageClassName: ot-cdh-sts-dancer-run-sc
...




---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: ot-cdh-sts-dancer-shr-pv1
  labels:
    release: stable
spec:
  capacity:
    storage: 1Gi
  accessModes: [ReadWriteOnce]
  persistentVolumeReclaimPolicy: Recycle
  hostPath:
    path: /run/ot-cdh-sts/dancer/pv1/shr
  storageClassName: ot-cdh-sts-dancer-shr-sc
...
---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: ot-cdh-sts-dancer-run-pv1
  labels:
    release: stable
spec:
  capacity:
    storage: 2Gi
  accessModes: [ReadWriteOnce]
  persistentVolumeReclaimPolicy: Recycle
  hostPath:
    path: /run/ot-cdh-sts/dancer/pv1/.run
  storageClassName: ot-cdh-sts-dancer-run-sc
...







---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: ot-cdh-sts-dancer-shr-pv2
  labels:
    release: stable
spec:
  capacity:
    storage: 1Gi
  accessModes: [ReadWriteOnce]
  persistentVolumeReclaimPolicy: Recycle
  hostPath:
    path: /run/ot-cdh-sts/dancer/pv2/shr
  storageClassName: ot-cdh-sts-dancer-shr-sc
...
---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: ot-cdh-sts-dancer-run-pv2
  labels:
    release: stable
spec:
  capacity:
    storage: 2Gi
  accessModes: [ReadWriteOnce]
  persistentVolumeReclaimPolicy: Recycle
  hostPath:
    path: /run/ot-cdh-sts/dancer/pv2/.run
  storageClassName: ot-cdh-sts-dancer-run-sc
...








---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: ot-cdh-sts-organizer-shr-pv0
  labels:
    release: stable
spec:
  capacity:
    storage: 1Gi
  accessModes: [ReadWriteOnce]
  persistentVolumeReclaimPolicy: Recycle
  hostPath:
    path: /run/ot-cdh-sts/organizer/pv0/shr
  storageClassName: ot-cdh-sts-organizer-shr-sc
...
---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: ot-cdh-sts-organizer-run-pv0
  labels:
    release: stable
spec:
  capacity:
    storage: 2Gi
  accessModes: [ReadWriteOnce]
  persistentVolumeReclaimPolicy: Recycle
  hostPath:
    path: /run/ot-cdh-sts/organizer/pv0/.run
  storageClassName: ot-cdh-sts-organizer-run-sc
...











###########################################################

# /etc/cloudera-scm-agent/config.ini

---
kind: ConfigMap
apiVersion: v1

metadata:
  name: etc-scm-agent-cfgmap
  namespace: ot-cdh-sts-ns

data:
  
  config.ini: |
    # Configuration file for cloudera-scm-agent.
    # Please note that this file supports multi-line values.  Multi-line
    # values are indicated by indenting following lines with a space.
    #
    # If you have whitespace in front of a parameter name, it will be
    # read as a continuation of the previous parameter value.  Please
    # be careful not to leave spaces in front of parameter names.
    #
    # To check if this file has spaces in front of parameters names
    # you can do a grep like this:
    #  grep '^[[:blank:]]' /etc/cloudera-scm-agent/config.ini

    [General]
    # Hostname of the CM server.
    server_host=ot-cdh-sts-organizer-0.ot-cdh-sts-organizer-svc-headless.ot-cdh-sts-ns.svc.cluster.local

    # Port that the CM server is listening on.
    server_port=7182

    ## It should not normally be necessary to modify these.
    # Port that the CM agent should listen on.
    # listening_port=9000

    # IP Address that the CM agent should listen on.
    # listening_ip=

    # Hostname that the CM agent reports as its hostname. If unset, will be
    # obtained in code through something like this:
    #
    #   python -c 'import socket; \
    #              print socket.getfqdn(), \
    #                    socket.gethostbyname(socket.getfqdn())'
    #
    # listening_hostname=

    # An alternate hostname to report as the hostname for this host in CM.
    # Useful when this agent is behind a load balancer or proxy and all
    # inbound communication must connect through that proxy.
    # reported_hostname=

    # Port that supervisord should listen on.
    # NB: This only takes effect if supervisord is restarted.
    # supervisord_port=19001

    # Log file.  The supervisord log file will be placed into
    # the same directory.  Note that if the agent is being started via the
    # init.d script, /var/log/cloudera-scm-agent/cloudera-scm-agent.out will
    # also have a small amount of output (from before logging is initialized).
    # log_file=/var/log/cloudera-scm-agent/cloudera-scm-agent.log

    # Persistent state directory.  Directory to store CM agent state that
    # persists across instances of the agent process and system reboots.
    # Particularly, the agent's UUID is stored here.
    # lib_dir=/var/lib/cloudera-scm-agent

    # Parcel directory.  Unpacked parcels will be stored in this directory.
    # Downloaded parcels will be stored in <parcel_dir>/../parcel-cache
    # parcel_dir=/opt/cloudera/parcels

    # Enable supervisord event monitoring.  Used in eager heartbeating, amongst
    # other things.
    # enable_supervisord_events=true

    # Maximum time to wait (in seconds) for all metric collectors to finish
    # collecting data.
    max_collection_wait_seconds=10.0

    # Maximum time to wait (in seconds) when connecting to a local role's
    # webserver to fetch metrics.
    metrics_url_timeout_seconds=30.0

    # Maximum time to wait (in seconds) when connecting to a local TaskTracker
    # to fetch task attempt data.
    task_metrics_timeout_seconds=5.0

    # The list of non-device (nodev) filesystem types which will be monitored.
    monitored_nodev_filesystem_types=nfs,nfs4,tmpfs

    # The list of filesystem types which are considered local for monitoring purposes.
    # These filesystems are combined with the other local filesystem types found in
    # /proc/filesystems
    local_filesystem_whitelist=ext2,ext3,ext4,xfs

    # The largest size impala profile log bundle that this agent will serve to the
    # CM server. If the CM server requests more than this amount, the bundle will
    # be limited to this size. All instances of this limit being hit are logged to
    # the agent log.
    impala_profile_bundle_max_bytes=1073741824

    # The largest size stacks log bundle that this agent will serve to the CM
    # server. If the CM server requests more than this amount, the bundle will be
    # limited to this size. All instances of this limit being hit are logged to the
    # agent log.
    stacks_log_bundle_max_bytes=1073741824

    # The size to which the uncompressed portion of a stacks log can grow before it
    # is rotated. The log will then be compressed during rotation.
    stacks_log_max_uncompressed_file_size_bytes=5242880

    # The orphan process directory staleness threshold. If a diretory is more stale
    # than this amount of seconds, CM agent will remove it.
    orphan_process_dir_staleness_threshold=5184000

    # The orphan process directory refresh interval. The CM agent will check the
    # staleness of the orphan processes config directory every this amount of
    # seconds.
    orphan_process_dir_refresh_interval=3600

    # A knob to control the agent logging level. The options are listed as follows:
    # 1) DEBUG (set the agent logging level to 'logging.DEBUG')
    # 2) INFO (set the agent logging level to 'logging.INFO')
    scm_debug=INFO

    # The DNS resolution collecion interval in seconds. A java base test program
    # will be executed with at most this frequency to collect java DNS resolution
    # metrics. The test program is only executed if the associated health test,
    # Host DNS Resolution, is enabled.
    dns_resolution_collection_interval_seconds=60

    # The maximum time to wait (in seconds) for the java test program to collect
    # java DNS resolution metrics.
    dns_resolution_collection_timeout_seconds=30

    # The directory location in which the agent-wide kerberos credential cache
    # will be created.
    # agent_wide_credential_cache_location=/var/run/cloudera-scm-agent

    [Security]
    # Use TLS and certificate validation when connecting to the CM server.
    use_tls=0

    # The maximum allowed depth of the certificate chain returned by the peer.
    # The default value of 9 matches the default specified in openssl's
    # SSL_CTX_set_verify.
    max_cert_depth=9

    # A file of CA certificates in PEM format. The file can contain several CA
    # certificates identified by
    #
    # -----BEGIN CERTIFICATE-----
    # ... (CA certificate in base64 encoding) ...
    # -----END CERTIFICATE-----
    #
    # sequences. Before, between, and after the certificates text is allowed which
    # can be used e.g. for descriptions of the certificates.
    #
    # The file is loaded once, the first time an HTTPS connection is attempted. A
    # restart of the agent is required to pick up changes to the file.
    #
    # Note that if neither verify_cert_file or verify_cert_dir is set, certificate
    # verification will not be performed.
    # verify_cert_file=

    # Directory containing CA certificates in PEM format. The files each contain one
    # CA certificate. The files are looked up by the CA subject name hash value,
    # which must hence be available. If more than one CA certificate with the same
    # name hash value exist, the extension must be different (e.g. 9d66eef0.0,
    # 9d66eef0.1 etc). The search is performed in the ordering of the extension
    # number, regardless of other properties of the certificates. Use the c_rehash
    # utility to create the necessary links.
    #
    # The certificates in the directory are only looked up when required, e.g. when
    # building the certificate chain or when actually performing the verification
    # of a peer certificate. The contents of the directory can thus be changed
    # without an agent restart.
    #
    # When looking up CA certificates, the verify_cert_file is first searched, then
    # those in the directory. Certificate matching is done based on the subject name,
    # the key identifier (if present), and the serial number as taken from the
    # certificate to be verified. If these data do not match, the next certificate
    # will be tried. If a first certificate matching the parameters is found, the
    # verification process will be performed; no other certificates for the same
    # parameters will be searched in case of failure.
    #
    # Note that if neither verify_cert_file or verify_cert_dir is set, certificate
    # verification will not be performed.
    # verify_cert_dir=

    # PEM file containing client private key.
    # client_key_file=

    # A command to run which returns the client private key password on stdout
    # client_keypw_cmd=

    # If client_keypw_cmd isn't specified, instead a text file containing
    # the client private key password can be used.
    # client_keypw_file=

    # PEM file containing client certificate.
    # client_cert_file=

    ## Location of Hadoop files.  These are the CDH locations when installed by
    ## packages.  Unused when CDH is installed by parcels.
    [Hadoop]
    #cdh_crunch_home=/usr/lib/crunch
    #cdh_flume_home=/usr/lib/flume-ng
    #cdh_hadoop_bin=/usr/bin/hadoop
    #cdh_hadoop_home=/usr/lib/hadoop
    #cdh_hbase_home=/usr/lib/hbase
    #cdh_hbase_indexer_home=/usr/lib/hbase-solr
    #cdh_hcat_home=/usr/lib/hive-hcatalog
    #cdh_hdfs_home=/usr/lib/hadoop-hdfs
    #cdh_hive_home=/usr/lib/hive
    #cdh_httpfs_home=/usr/lib/hadoop-httpfs
    #cdh_hue_home=/usr/share/hue
    #cdh_hue_plugins_home=/usr/lib/hadoop
    #cdh_impala_home=/usr/lib/impala
    #cdh_kudu_home=/usr/lib/kudu
    #cdh_llama_home=/usr/lib/llama
    #cdh_mr1_home=/usr/lib/hadoop-0.20-mapreduce
    #cdh_mr2_home=/usr/lib/hadoop-mapreduce
    #cdh_oozie_home=/usr/lib/oozie
    #cdh_parquet_home=/usr/lib/parquet
    #cdh_pig_home=/usr/lib/pig
    #cdh_solr_home=/usr/lib/solr
    #cdh_spark_home=/usr/lib/spark
    #cdh_sqoop_home=/usr/lib/sqoop
    #cdh_sqoop2_home=/usr/lib/sqoop2
    #cdh_yarn_home=/usr/lib/hadoop-yarn
    #cdh_zookeeper_home=/usr/lib/zookeeper
    #hive_default_xml=/etc/hive/conf.dist/hive-default.xml
    #webhcat_default_xml=/etc/hive-webhcat/conf.dist/webhcat-default.xml
    #jsvc_home=/usr/libexec/bigtop-utils
    #tomcat_home=/usr/lib/bigtop-tomcat
    #oracle_home=/usr/share/oracle/instantclient

    ## Location of Cloudera Management Services files.
    [Cloudera]
    #mgmt_home=/usr/share/cmf

    ## Location of JDBC Drivers.
    [JDBC]
    #cloudera_mysql_connector_jar=/usr/share/java/mysql-connector-java.jar
    #cloudera_oracle_connector_jar=/usr/share/java/oracle-connector-java.jar
    #By default, postgres jar is found dynamically in $MGMT_HOME/lib
    #cloudera_postgresql_jdbc_jar=

    [Cgroup_Paths]
    # This section lists paths that agent will use to figure out resource allocation
    # If not specified then agent will use the cgroup that agent process is part of
    #
    # Note: do not add /sys/fs/cgroup in the path. cgroups mounted inside container will
    # be used to figure out the subsystems.
    #cpu_cgroup_path=/
    #cpuacct_cgroup_path=/
    #blkio_cgroup_path=/
    #memory_cgroup_path=/
...








###########################################################

## dcr

---
kind: Service
apiVersion: v1

metadata:
  
  name: ot-cdh-sts-dancer-svc-headless
  namespace: ot-cdh-sts-ns
  
  labels:
    app: ot-cdh-sts
    role: dancer-svc-headless
    release: dev

spec:
  
  clusterIP: None
  
  selector:
    app: ot-cdh-sts
    role: dancer-p
    release: dev
  
  ports:
  - name: m-p7182
    port: 7182
    targetPort: 7182
  - name: m-p3306
    port: 3306
    targetPort: 3306
  - name: m-p9870
    port: 9870
    targetPort: 9870
  - name: m-p50070
    port: 50070
    targetPort: 50070
  - name: m-p8080
    port: 8080
    targetPort: 8080
  - name: m-p8180
    port: 8180
    targetPort: 8180
  - name: m-p9092
    port: 9092
    targetPort: 9092
  - name: m-p2181
    port: 2181
    targetPort: 2181
  - name: m-p10000
    port: 10000
    targetPort: 10000
...


---
kind: StatefulSet
apiVersion: apps/v1

metadata:
  
  name: ot-cdh-sts-dancer
  namespace: ot-cdh-sts-ns
  
  labels:
    app: ot-cdh-sts
    role: dancer
    release: dev

spec:
  
  replicas: 3
  serviceName: ot-cdh-sts-dancer-svc-headless
  
  selector:
    
    matchLabels:
      app: ot-cdh-sts
      role: dancer-p
      release: dev
  
  template:
    
    metadata:
      
      labels:
        app: ot-cdh-sts
        role: dancer-p
        release: dev
    
    spec:
      
      volumes:
        
      - name: etc-scm-agent
        
        configMap: 
          name: etc-scm-agent-cfgmap
          defaultMode: 0666
      
      containers:
        
      - name: ot-cdh-sts-dancer-p-c
        image: harbor.io/vmcontainer/centos7:0.2-mdbconnectordl5cli
        
        command: [/usr/sbin/init]
        
        lifecycle:
          postStart: {exec: {command: [/bin/sh,-c,rm -f -- /var/lib/cloudera-scm-agent/uuid /var/run/cloudera-scm-agent.pid]}}
        
        ports:
        - name: port-7180
          containerPort: 7180
        - name: port-7182
          containerPort: 7182
        - name: port-9000
          containerPort: 9000
        - name: port-9001
          containerPort: 9001
        - name: port-3306
          containerPort: 3306
        - name: port-9870
          containerPort: 9870
        - name: port-50070
          containerPort: 50070
        - name: port-8080
          containerPort: 8080
        - name: port-8180
          containerPort: 8180
        - name: port-8022
          containerPort: 8022
        - name: port-9092
          containerPort: 9092
        - name: port-2181
          containerPort: 2181
        
        volumeMounts:
          
        #- name: etc-scm-agent
          #mountPath: /etc/cloudera-scm-agent
          
          ## 上面这个是为了抽取 /etc/cloudera-scm-agent/config.ini 而做的。（意思是随时欢迎更优雅完成这个目的的替代方案）
          ## 该方案目前看暂且认为可行：因为虽然这会消灭这个目录的所有文件，但这个目录本来也只有 config.ini 那一个文件。
          ## 已暂时弃用，取消注释上面代码就能再启用本案。
          
        - name: ot-cdh-sts-dancer-shr
          mountPath: /usr/share/ot-cdh-sts-dancer
        - name: ot-cdh-sts-dancer-run
          mountPath: /run
  
  volumeClaimTemplates:
    
  - metadata: {name: ot-cdh-sts-dancer-shr}
    spec:
      accessModes: [ReadWriteOnce]
      storageClassName: ot-cdh-sts-dancer-shr-sc
      resources:
        requests: {storage: 1Gi}
    
  - metadata: {name: ot-cdh-sts-dancer-run}
    spec:
      accessModes: [ReadWriteOnce]
      storageClassName: ot-cdh-sts-dancer-run-sc
      resources:
        requests: {storage: 2Gi}
...








# ot-cdh-sts-dancer-[0-2].ot-cdh-sts-dancer-svc-headless.ot-cdh-sts-ns.svc.cluster.local
# http://10.28.3.14/cm6.2/
# http://10.28.3.14/cdh6.2/


# urls () { seq 0 2 | xargs -i -- echo ot-cdh-sts-dancer-{}.ot-cdh-sts-dancer-svc-headless.ot-cdh-sts-ns.svc.cluster.local ; } && export URLS="$(urls)" ;
# runs () { urls | xargs -i -P0 -- sshpass -p "$PSWD" -- ssh -o StrictHostKeyChecking=no  '{}'  "$@" ; } ;
# runs -- test -f /etc/hosts-orig '||' '{' awk '!'/'$'HOSTNAME/ /etc/hosts '>' /etc/hosts-orig ';' '}' '&&' '(' cat /etc/hosts-orig '&&' echo "'$(runs -- awk /'$'HOSTNAME/ /etc/hosts)'" ')' '>>' /etc/hosts
# runs -- cat /etc/hosts



###########################################################

## org

---
kind: Service
apiVersion: v1

metadata:
  
  name: ot-cdh-sts-organizer-svc-headless
  namespace: ot-cdh-sts-ns
  
  labels:
    app: ot-cdh-sts
    role: organizer-svc-headless
    release: dev

spec:
  
  clusterIP: None
  
  selector:
    app: ot-cdh-sts
    role: organizer-p
    release: dev
  
  ports:
  - name: m-p7180
    port: 7180
    targetPort: 7180
  - name: m-p3306
    port: 3306
    targetPort: 3306
  - name: m-p8088
    port: 8088
    targetPort: 8088
  - name: m-p50070
    port: 50070
    targetPort: 50070
  - name: m-p8080
    port: 8080
    targetPort: 8080
  - name: m-p8180
    port: 8180
    targetPort: 8180
  - name: m-p9092
    port: 9092
    targetPort: 9092
  - name: m-p2181
    port: 2181
    targetPort: 2181
  - name: m-p10000
    port: 10000
    targetPort: 10000
...


---
kind: Service
apiVersion: v1

metadata:
  
  name: ot-cdh-sts-organizer-svc-nodeport
  namespace: ot-cdh-sts-ns
  
  labels:
    app: ot-cdh-sts
    role: organizer-svc-nodeport
    release: dev

spec:
  
  type: NodePort
  
  selector:
    app: ot-cdh-sts
    role: organizer-p
    release: dev
  
  ports:
  - name: p22
    port: 22
    targetPort: 22
    nodePort: 31000
  - name: p7180
    port: 7180
    targetPort: 7180
    nodePort: 31001
  - name: p3306
    port: 3306
    targetPort: 3306
    nodePort: 31002
  - name: p8088
    port: 8088
    targetPort: 8088
    nodePort: 31003
  - name: p50070
    port: 50070
    targetPort: 50070
    nodePort: 31004
  - name: p8080
    port: 8080
    targetPort: 8080
    nodePort: 31005
  - name: p8180
    port: 8180
    targetPort: 8180
    nodePort: 31006
  - name: p9092
    port: 9092
    targetPort: 9092
    nodePort: 31007
  - name: p2181
    port: 2181
    targetPort: 2181
    nodePort: 31008
  - name: p10000
    port: 10000
    targetPort: 10000
    nodePort: 31009
  - name: p19888
    port: 19888
    targetPort: 19888
    nodePort: 31010
...






---
kind: ConfigMap
apiVersion: v1

metadata:
  
  name: nodes-init-scripts-cfgmap
  namespace: ot-cdh-sts-ns
  
  labels:
    app: ot-cdh-sts
    role: nodes-init-scripts-cfgmap
    release: dev

data:
  
  launcher-with-bash.sh: |
    #! /bin/sh
    
    # suggest to use a installed img !
    # if want to install online per restart, u can use this script :
    
    # when bash is installed, apk add bash will not run ;
    # when bash is not installed, apk add bash will run after repositories changed to ustc .
    
    sed -i.bak1 s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g /etc/apk/repositories &&
    while (apk info bash && { exit 1 ; } || { exit 0 ; }) ; do apk add bash ; done ;
    
    echo '-------------------------------' &&
    exec /usr/bin/env bash "$@" ;
  
  nodes-init.sh: |
    #! /bin/bash
    
    ## wait for runs need env: PSWD METADB_PSWD
    
    say_envs () { env | awk "/$(echo "$*" | xargs -n1 | xargs -i -- echo ^{}= | xargs | tr -- \  \|)/" ; } &&
    
    while sleep 4s ;
    do 
        case "${PSWD}${METADB_PSWD}"
        in
            '') echo :no :passwords :all ; say_envs PSWD METADB_PSWD ;;
            "$PSWD") echo :no :passwords METADB_PSWD ; say_envs PSWD METADB_PSWD ;;
            "$METADB_PSWD") echo :no :passwords PSWD ; say_envs PSWD METADB_PSWD ;;
            *) echo :yes :passwords :odk ; say_envs PSWD METADB_PSWD ; break ;;
        esac ;
    done ;
    
    
    
    
    
    
    
    
    
    
    
    ## add tools
    sed -i.bak2 s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g /etc/apk/repositories &&
    apk add  openssh sshpass expect &&
    
    
    
    ## mk keygen, for unpass
    expect -c 'spawn ssh-keygen -t rsa ; expect "Enter" {send "\r";exp_continue} "Overwrite" {send "y\r";exp_continue}' ;
    
    
    
    
    
    ## nodes funs
    
    urls () { seq 0 2 | xargs -i -- echo ot-cdh-sts-dancer-{}.ot-cdh-sts-dancer-svc-headless.ot-cdh-sts-ns.svc.cluster.local ; } && export URLS="$(urls)" &&
    runs () { urls | xargs -i -P0 -- sshpass -p "$PSWD" -- ssh -o StrictHostKeyChecking=no  '{}'  "$@" ; } &&
    gets () { runs -- awk /'$'HOSTNAME/ /etc/hosts ; } ;
    
    
    
    
    
    ##[zan shi bu yong] [ke zuo can kao]##
    #>> # fixs () { runs -- test -f /etc/hosts-orig '||' '{' awk '!'/'$'HOSTNAME/ /etc/hosts '>' /etc/hosts-orig ';' '}' '&&' '(' cat /etc/hosts-orig '&&' echo "'$(gets)'" ')' '>>' /etc/hosts ; } ;
    #>> # sees () { runs -- cat /etc/hosts ; } ;
    
    
    
    
    
    
    ## repo get defs
    
    repo_gets ()
    {
        runs -- "echo '$(
            wget -qO - -- http://ot-cdh-sts-repo-0.ot-cdh-sts-repo-svc-headless.ot-cdh-sts-ns.svc.cluster.local:80/repofiles/cm6.repo | tee /etc/.shr/cm6.repo
            )' > /etc/yum.repos.d/cm6.repo" ;
    } &&
    
    repo_gets_retring ()
    {
        repo_gets &&
        { echo :ok :repoget :done ${1:-0} ; } ||
        { echo :fail :repoget :retried ${1:-0} ; exec bash -c "
            $(declare -f repo_gets) &&
            $(declare -f repo_gets_retring) ; sleep 1s ; repo_gets_retring $(($1 + 1))" ; } ;
    } ;
    
    
    ## metadb fun defs
    
    mainc_dosth () { sshpass -p "$PSWD" -- ssh -o StrictHostKeyChecking=no localhost "$@" ; } &&
    
    mainc_metadbin () { mainc_dosth -- mysql -h ot-cdh-sts-metadb-svc-clusterip.ot-cdh-sts-ns.svc.cluster.local "$@" ; } &&
    
    addin_meta_mysql ()
    {
        outsqlmk_declaring () 
        {
            echo 'create database if not exists `{}` DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci ;' &&
            echo 'grant all on `{}`.* to "{}"@"%" identified by "{}-pass&" ;' &&
            echo 'show grants for "{}"@"%" ;' ;
        } &&
        
        (echo "$@" | xargs -n1) |
            xargs -i -P0 -- bash -c "$(declare -f outsqlmk_declaring) && outsqlmk_declaring" |
            mainc_metadbin -u root -p"'$METADB_PSWD'" ;
    } ;
    
    
    
    
    
    
    
    # -----...----- #
    
    
    
    ## wait runs can run
    while sleep 16s; do runs -- hostname && { echo :ok :nodes :done ; break ; } ; done ;
    
    
    
    
    ## wait meta repo urls
    oth_urls () { echo ot-cdh-sts-metadb-svc-clusterip.ot-cdh-sts-ns.svc.cluster.local ; echo ot-cdh-sts-repo-0.ot-cdh-sts-repo-svc-headless.ot-cdh-sts-ns.svc.cluster.local ; } &&
    while sleep 8s; do (oth_urls | xargs -i -- ping -c 4 -i 0.4 -w 2 -- {}) && { echo :ok :repo-and-metadb :done ; break ; } ; done ;

    
    
    
    # ------------- #
    
    
    
    
    
    ## metadb init - bigdatas
    
    retring_addin_meta_mysql ()
    {
        (addin_meta_mysql  hive hbase spark  oozie hue  amon rman  scm nav navms sentry) &>> /etc/.shr/addin_meta_mysql.log &&
        
        { echo :ok :retring_addin_meta_mysql :retried ${1:-0} times ; } ||
        { echo :err :retring_addin_meta_mysql :retried ${1:-0} times ; exec bash -c "
            
            $(declare -f mainc_dosth) && $(declare -f mainc_metadbin) && $(declare -f addin_meta_mysql) &&
            
            $(declare -f retring_addin_meta_mysql) ; sleep 1s ; retring_addin_meta_mysql $(($1 + 1))" ; } ;
    } &&
    
    (retring_addin_meta_mysql 0) && echo :done :bigdatas :initmeta || echo :fail :bigdatas :initmeta ;
    
    
    ## metadb init - scm
    
    #(echo 'grant all on `scm`.* to "scm"@"%" identified by "scm-pass&" ;' | mainc_metadbin -u root -p"'$METADB_PSWD'") &&
    ### 试试可行，就去掉这行，还有下面那个 scm 自己的鬼脚本里的那2行。
    ### 依据： https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/prepare_cm_database.html
    
    
    ## repo get should done before yumin at main-c
    
    (repo_gets_retring 0) &&
    
    
    ## install & init - the manager (scm-server.service and so on...)
    ## --- infact, we need to volume all things that the  install & init  might changes !!
    
    (
        mainc_dosth -- '
            
            cat /etc/.shr-c/cm6.repo > /etc/yum.repos.d/cm6.repo &&
            
            mngr_in_retring ()
            {
                (
                    echo ============ yum-in - retried ${1:-0} times ============= &&
                    (yum clean all && rpm --rebuilddb && yum makecache) &&
                    yum install -y oracle-j2sdk1.8.x86_64 cloudera-manager-server
                ) &>> mngr_in_retring.log &&
                { echo :ok ::$? :mngr-in ${1:-0} ; } ||
                { echo :err ::$? :mngr-in ${1:-0} ; exec bash -c "
                    
                    $(declare -f mngr_in_retring) ; sleep 1s ; mngr_in_retring $(($1 + 1))" ; } ;
            } &&
            
            (mngr_in_retring 0) &&
            
            # scm meta
            # useage, see: /opt/cloudera/cm/schema/scm_prepare_database.sh --help
            # this sh will be here after the install before here.
            
            (
                /opt/cloudera/cm/schema/scm_prepare_database.sh \
                    --host=ot-cdh-sts-metadb-svc-clusterip.ot-cdh-sts-ns.svc.cluster.local \
                    --scm-host=ot-cdh-sts-organizer-0.ot-cdh-sts-organizer-svc-headless.ot-cdh-sts-ns.svc.cluster.local \
                    -- mysql scm scm scm-pass\&  &&
                
                { echo :ok :scm_prepare_database ::$? ; } ||
                { echo :err :scm_prepare_database ::$? ; }
            ) &> scm_prepare_database.log &&
            
            # start srv
            systemctl enable cloudera-scm-server.service &&
            systemctl restart cloudera-scm-server.service ' &&
        
        { echo :done ::$? :scm-server :init ; } ||
        { echo :fail ::$? :scm-server :init ; exit 3 ; }
    ) &> /etc/.shr/installinit-scmmanager.log &
    
    
    

    
    
    # ------------- #
    
    
    
    ## mamo: unpass & hosts - doses
    
    unpass_exp_declaring ()
    { expect -c 'spawn ssh-copy-id root@{};expect "yes/no" {send "yes\r";exp_continue} "password" {send "'"$PSWD"'\r"};expect eof' ; } &&
    
    echo ':: mamo-ing ...'
    
    while (sleep 1m ; n="$(expr ${n:-0} + 1)" && echo ">>>>> times: $n, date: $(date)") ;
    do
        
        ## unpass them
        (
            urls | xargs -i -P0 -- bash -c "
                $(declare -f unpass_exp_declaring) &&
                unpass_exp_declaring &> /etc/.shr/unpass_exp_declaring.log &&
                { echo :ok :unpass for {} ; } ||
                { echo :fail :unpass for {} ; } "
        ) &&
        
        
        
        ## chg self hosts
        (
            (test -f /etc/.shr/hosts-orig || cat /etc/hosts > /etc/.shr/hosts-orig) &&
            (cat /etc/.shr/hosts-orig && echo && gets) > /etc/hosts &&
            { echo :ok :hostschg :self ; } ||
            { echo :err :hostschg :self ; }
        ) &&
        
        
        
        ## chg dcrs hosts (for DNS - Fan Xiang Jian Cha)
        
        ##(some cluster need all-have-all, this is all-have-leader)##
        
        #runs -- "(test -f /etc/hosts-dcr0 || cat /etc/hosts > /etc/hosts-dcr0) && (cat /etc/hosts-dcr0 && echo && echo '$(fgrep $HOSTNAME /etc/hosts)') > /etc/hosts" &&
        
        ##(this is all-sameas-leader)##
        
        (
            runs -- "
                (test -f /etc/hosts-dcr0 || cat /etc/hosts > /etc/hosts-dcr0) &&
                (echo '$(cat /etc/hosts)') > /etc/hosts " &&
            { echo :ok :hostschg :nodes ; } ||
            { echo :err :hostschg :nodes ; }
        ) ;
        
        
        
        ## add repo
        (repo_gets && echo :ok :repoget :mamo || echo :err :repoget :mamo) ;
        
        ## log clear
        (echo "$(tail -n 1700 -- /etc/.shr/mamo-ing.log)" > /etc/.shr/mamo-ing.log) ;
        
    done &> /etc/.shr/mamo-ing.log ;
...





---
kind: StatefulSet
apiVersion: apps/v1

metadata:
  
  name: ot-cdh-sts-organizer
  namespace: ot-cdh-sts-ns
  
  labels:
    app: ot-cdh-sts
    role: organizer
    release: dev

spec:
  
  serviceName: ot-cdh-sts-organizer-svc-headless
  replicas: 1
  
  selector:
    matchLabels:
      app: ot-cdh-sts
      role: organizer-p
      release: dev
  
  volumeClaimTemplates:
    
  - metadata:
      name: ot-cdh-sts-organizer-shr-volume
    
    spec:
      accessModes: [ReadWriteOnce]
      resources:
        requests:
          storage: 1Gi
      storageClassName: ot-cdh-sts-organizer-shr-sc
    
  - metadata:
      name: ot-cdh-sts-organizer-run-volume
    
    spec:
      accessModes: [ReadWriteOnce]
      resources:
        requests:
          storage: 2Gi
      storageClassName: ot-cdh-sts-organizer-run-sc
  
  template:
    
    metadata:
      
      labels:
        app: ot-cdh-sts
        role: organizer-p
        release: dev
    
    spec:
      
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms: # 指定节点
            - matchExpressions: # 下面的条件指定节点往 dockerapache-04 调度
              - key: kubernetes.io/hostname
                operator: In
                values: [dockerapache-04]
      
      volumes:
        
      - name: etc-scm-agent
        configMap: {name: etc-scm-agent-cfgmap, defaultMode: 0666}
        
      - name: nodes-init-scripts
        configMap: {name: nodes-init-scripts-cfgmap}
        
      - name: etc-shr
      - name: ssh-dir
      
      containers:
        
      - name: ot-cdh-sts-organizer-p-c
        
        ## 本容器扮演主节点的角色（组织者）
        ## 还有一个容器相当于辅助本容器，会定时地动态更新本容器的 hosts 文件以及随时分发公钥给从节点。
        ## （从节点是谁目前尚且是写死了而且尚且是不便配置的）
        
        image: harbor.io/vmcontainer/centos7:0.2-mdbconnectordl5cli
        command: [/usr/sbin/init]
        
        ports:
          
        ## 'Number of port to expose on the pod's IP address.': 
        ## 应该是令这个 Pod 的被访问的 port 以这个 容器 为准。因为不同容器在同一个网（据说是）。
        
        - name: port-22
          containerPort: 22
        - name: port-7180
          containerPort: 7180
        - name: port-7182
          containerPort: 7182
        - name: port-9000
          containerPort: 9000
        - name: port-9001
          containerPort: 9001
        - name: port-3306
          containerPort: 3306
        - name: port-9870
          containerPort: 9870
        - name: port-50070
          containerPort: 50070
        - name: port-8080
          containerPort: 8080
        - name: port-8180
          containerPort: 8180
        - name: port-8022
          containerPort: 8022
        - name: port-9092
          containerPort: 9092
        - name: port-2181
          containerPort: 2181
        
        volumeMounts:
          
        #- name: etc-scm-agent
          #mountPath: /etc/cloudera-scm-agent
          
          ## 上面这个是为了抽取 /etc/cloudera-scm-agent/config.ini 而做的。（意思是随时欢迎更优雅完成这个目的的替代方案）
          ## 该方案目前看暂且认为可行：因为虽然这会消灭这个目录的所有文件，但这个目录本来也只有 config.ini 那一个文件。
          ## 已暂时弃用，取消注释上面代码就能再启用本案。
          
        - name: ot-cdh-sts-organizer-shr-volume
          mountPath: /usr/share/ot-cdh-sts-organizer
        - name: ot-cdh-sts-organizer-run-volume
          mountPath: /run
          
        - name: etc-shr
          mountPath: /etc/.shr-c
        - name: ssh-dir
          mountPath: /root/.ssh
        
      - name: ot-cdh-sts-organizer-tools
        
        image: harbor.io/tools/mamo-tools:alpine-1.0
        
        envFrom: [{configMapRef: {name: passwordenvs-cfgmap}}]
        
        command: [sh,--,/root/nodes/init-scripts/launcher-with-bash.sh]
        args: [--,/root/nodes/init-scripts/nodes-init.sh]
          
        volumeMounts:
        
        - name: nodes-init-scripts
          mountPath: /root/nodes/init-scripts
          
          ## 上面这个用来把之前的脚本抽出到 cfgmap 里
          
        - name: etc-shr
          mountPath: /etc/.shr
          
          ## 其实上面这个只是用来把备份的原始 hosts 文件
          ## 来让主要的那个容器里也能看见（但是也能更改了这不好）
          
        - name: ssh-dir
          mountPath: /root/.ssh
          
          ## 上面这个就是同步一下生成的密钥，实际生成密钥和分发都是在这个容器做的。
          ## 这里同步一下就是为了把辅助的狸猫换成真正的太子（嗯哼这跟狸猫换太子的成语意思相反了）。
...




## 会有下面这俩玩意儿
## 是因为之前我把上面那玩意儿给变成过 deployment 。

#---
#kind: PersistentVolumeClaim
#apiVersion: v1
#metadata:
  #name: ot-cdh-sts-organizer-shr-pvc
  #namespace: ot-cdh-sts-ns
#spec:
  #accessModes: [ReadWriteOnce]
  #resources:
    #requests:
      #storage: 1Gi
  #storageClassName: ot-cdh-sts-organizer-shr-sc
#...
#---
#kind: PersistentVolumeClaim
#apiVersion: v1
#metadata:
  #name: ot-cdh-sts-organizer-run-pvc
  #namespace: ot-cdh-sts-ns
#spec:
  #accessModes: [ReadWriteOnce]
  #resources:
    #requests:
      #storage: 2Gi
  #storageClassName: ot-cdh-sts-organizer-run-sc
#...






#---
#apiVersion: extensions/v1beta1
#kind: Ingress
#metadata:
  #name: ot-cdh-sts-organizer-svc-headless-ingress
  #namespace: ot-cdh-sts-ns
  #labels:
    #ingresscontroller: haproxy
    #host: ot-cdh-sts.organizer.svc
  #annotations:
    #kubernetes.io/ingress.class: haproxy
#spec:
  ##ingressClassName: haproxy
  #backend:
    #serviceName: ot-cdh-sts-organizer-svc-headless
    #servicePort: 7180
#...

## 这玩意儿的使用体验很糟糕，它甚至都不让我指定清楚我外面要访问哪个节点。
## 而且这个代理的能力很有限，不是所有的 Web 服务都能这样简单代理（简单的可以用它）。



################################################################

# communicator

---
kind: ConfigMap
apiVersion: v1

metadata:
  
  name: ngx-conf-otstscdh
  namespace: ot-cdh-sts-ns

data:
  
  ot-cdh-sts-orgr.conf: |
    server {
        listen   30077 ;
        server_name   ~^10\.28\.3\.1.$ ;
        location   /  {
            #proxy_pass    http://ot-cdh-sts-organizer-0.ot-cdh-sts-organizer-svc-headless.ot-cdh-sts-ns.svc.cluster.local:7180/ ;
        }
    }
    server {
        listen   30078 ;
        server_name   ~^10\.28\.3\.1.$ ;
        location   /  {
            proxy_pass    http://ot-cdh-sts-dancer-0.ot-cdh-sts-dancer-svc-headless.ot-cdh-sts-ns.svc.cluster.local:7180/ ;
        }
    }
    server {
        listen   30178 ;
        server_name   ~^10\.28\.3\.1.$ ;
        location   /  {
            proxy_pass    http://ot-cdh-sts-dancer-1.ot-cdh-sts-dancer-svc-headless.ot-cdh-sts-ns.svc.cluster.local:7180/ ;
        }
    }
    server {
        listen   30278 ;
        server_name   ~^10\.28\.3\.1.$ ;
        location   /  {
            proxy_pass    http://ot-cdh-sts-dancer-2.ot-cdh-sts-dancer-svc-headless.ot-cdh-sts-ns.svc.cluster.local:7180/ ;
        }
    }
...


## 上面这玩意在一些图形界面上就可以像编辑配置文件一样。
## 不过它在这还并不是文件；
## 而是在下面 po.spec.volumes.configMap 那里才所在资源被视为文件。


---
kind: Deployment
apiVersion: apps/v1

metadata:
  
  name: ot-cdh-sts-communicator
  namespace: ot-cdh-sts-ns
  
  labels:
    app: ot-cdh-sts
    role: communicator
    release: dev

spec:
  
  replicas: 4
  
  selector:
    
    matchLabels:
      app: ot-cdh-sts
      role: communicator-p
      release: dev
  
  template:
    
    metadata:
      
      labels:
        app: ot-cdh-sts
        role: communicator-p
        release: dev
    
    spec:
      
      hostname: ot-cdh-sts-communicator
      
      volumes:
        
      - name: ngx-conf
        
        configMap:
          name: ngx-conf-otstscdh
          defaultMode: 420
      
      containers:
        
      - name: ot-cdh-sts-communicator-p-c
        
        image: docker.io/nginx:latest
        
        volumeMounts:
          
        - name: ngx-conf
          mountPath: /etc/nginx/conf.d
        
        ports:
        - name: ngx-default
          containerPort: 80
        - name: ngx-organizer-0
          containerPort: 30077
        - name: ngx-dancer-0
          containerPort: 30078
        - name: ngx-dancer-1
          containerPort: 30178
        - name: ngx-dancer-2
          containerPort: 30278
      
      affinity:
        
        nodeAffinity:
          
          requiredDuringSchedulingIgnoredDuringExecution:
            
            nodeSelectorTerms:
              
            - matchExpressions:
                
              - key: kubernetes.io/hostname
                operator: In
                values:
                - dockerapache-04
                - dockerapache-03
...
---
kind: Service
apiVersion: v1

metadata:
  
  name: ot-cdh-sts-communicator-svc-nodeport
  namespace: ot-cdh-sts-ns
  
  labels:
    app: ot-cdh-sts
    role: communicator-svc-nodeport
    release: dev

spec:
  
  type: NodePort
  
  selector:
    app: ot-cdh-sts
    role: communicator-p
    release: dev
  
  ports:
  - name: port-80
    port: 80
    targetPort: 80
    nodePort: 30080
  - name: port-end
    port: 32767
    targetPort: 32767
    nodePort: 32767
  - name: organizer-0
    port: 30077
    targetPort: 30077
    nodePort: 30077
  - name: dancer-0
    port: 30078
    targetPort: 30078
    nodePort: 30078
  - name: dancer-1
    port: 30178
    targetPort: 30178
    nodePort: 30178
  - name: dancer-2
    port: 30278
    targetPort: 30278
    nodePort: 30278
...


## 重大发现：服务的选择器应当对应豆荚的标签！！！而不是部署或副本集的标签！！！
## （之所以叫重大发现是因为竟然没有任何一个示例体现出这一点；大家都是把能跑就行给来回复制的吗。。。）




###################################################

## 几个对镜像本身要做的事儿：
## 0. 增加该增加的应用。
## 1. 本来的文件（配置或日志）可以先改名备份。
## 2. 数据库元数据工作能提前做提前做。
## 3. 命令的历史也很重要。也别是，最好能到时候进入新的 Pod 调试的时候一按上就是需要用的。

## 新的镜像：
## 0. 尽量用空镜像、需要安装的都搞到自制源（尽量只包括 CDH 及其依赖）
## 1. 在已经不需要初始化的时候初始化程序不会产生影响

## bash fun declaring
## - for xargs: xxx_declaring
## - for retry: xxx_retring
## - all deps of declared-fun also need to be declared , so all funs need to be declarable !!
##   (this is same as closure but we need do it manually on bash ...)




